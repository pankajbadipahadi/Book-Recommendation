{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Building**"
      ],
      "metadata": {
        "id": "7Dch2_nWNnXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y numpy pandas scikit-learn scikit-surprise\n",
        "!pip install numpy==1.26.4  # Install a compatible NumPy 1.x version for Python 3.12\n",
        "!pip install pandas scikit-learn scikit-surprise --no-deps # Reinstall dependent libraries ensuring compatibility\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "from surprise import Dataset, Reader, SVD, KNNBaseline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KgIT6N1Zs54p",
        "outputId": "26ab3cb2-c5d1-409c-a4ff-dbb5f8c9b78b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "Found existing installation: pandas 2.3.3\n",
            "Uninstalling pandas-2.3.3:\n",
            "  Successfully uninstalled pandas-2.3.3\n",
            "Found existing installation: scikit-learn 1.8.0\n",
            "Uninstalling scikit-learn-1.8.0:\n",
            "  Successfully uninstalled scikit-learn-1.8.0\n",
            "Found existing installation: scikit-surprise 1.1.4\n",
            "Uninstalling scikit-surprise-1.1.4:\n",
            "  Successfully uninstalled scikit-surprise-1.1.4\n",
            "Collecting numpy==1.26.4\n",
            "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "Installing collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tsfresh 0.21.1 requires pandas>=0.25.0, which is not installed.\n",
            "tsfresh 0.21.1 requires scikit-learn>=0.22.0, which is not installed.\n",
            "tensorflow-decision-forests 1.12.0 requires pandas, which is not installed.\n",
            "pointpats 2.5.2 requires pandas!=1.5.0,>=1.4, which is not installed.\n",
            "cmdstanpy 1.3.0 requires pandas, which is not installed.\n",
            "datasets 4.0.0 requires pandas, which is not installed.\n",
            "prophet 1.2.1 requires pandas>=1.0.4, which is not installed.\n",
            "mlxtend 0.23.4 requires pandas>=0.24.2, which is not installed.\n",
            "mlxtend 0.23.4 requires scikit-learn>=1.3.1, which is not installed.\n",
            "pymc 5.26.1 requires pandas>=0.24.0, which is not installed.\n",
            "tobler 0.12.1 requires pandas, which is not installed.\n",
            "mapclassify 2.10.0 requires pandas>=2.1, which is not installed.\n",
            "mapclassify 2.10.0 requires scikit-learn>=1.4, which is not installed.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, which is not installed.\n",
            "statsmodels 0.14.6 requires pandas!=2.1.0,>=1.4, which is not installed.\n",
            "hdbscan 0.8.40 requires scikit-learn>=0.20, which is not installed.\n",
            "libpysal 4.13.0 requires pandas>=1.4, which is not installed.\n",
            "libpysal 4.13.0 requires scikit-learn>=1.1, which is not installed.\n",
            "dopamine-rl 4.1.2 requires pandas>=0.24.2, which is not installed.\n",
            "yfinance 0.2.66 requires pandas>=1.3.0, which is not installed.\n",
            "segregation 2.5.3 requires pandas, which is not installed.\n",
            "segregation 2.5.3 requires scikit-learn>=0.21.3, which is not installed.\n",
            "xarray 2025.12.0 requires pandas>=2.2, which is not installed.\n",
            "spreg 1.8.4 requires pandas, which is not installed.\n",
            "spreg 1.8.4 requires scikit-learn>=0.22, which is not installed.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, which is not installed.\n",
            "pandas-gbq 0.30.0 requires pandas>=1.1.4, which is not installed.\n",
            "arviz 0.22.0 requires pandas>=2.1.0, which is not installed.\n",
            "spopt 0.7.0 requires pandas>=2.1.0, which is not installed.\n",
            "spopt 0.7.0 requires scikit-learn>=1.4.0, which is not installed.\n",
            "holoviews 1.22.1 requires pandas>=1.3, which is not installed.\n",
            "bigframes 2.30.0 requires pandas>=1.5.3, which is not installed.\n",
            "cufflinks 0.17.3 requires pandas>=0.19.2, which is not installed.\n",
            "geemap 0.35.3 requires pandas, which is not installed.\n",
            "bqplot 0.12.45 requires pandas<3.0.0,>=1.0.0, which is not installed.\n",
            "shap 0.50.0 requires pandas, which is not installed.\n",
            "shap 0.50.0 requires scikit-learn, which is not installed.\n",
            "sklearn-pandas 2.2.0 requires pandas>=1.1.4, which is not installed.\n",
            "sklearn-pandas 2.2.0 requires scikit-learn>=0.23.0, which is not installed.\n",
            "bokeh 3.7.3 requires pandas>=1.2, which is not installed.\n",
            "sentence-transformers 5.1.2 requires scikit-learn, which is not installed.\n",
            "spaghetti 1.7.6 requires pandas!=1.5.0,>=1.4, which is not installed.\n",
            "pynndescent 0.5.13 requires scikit-learn>=0.18, which is not installed.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, which is not installed.\n",
            "fastai 2.8.5 requires pandas, which is not installed.\n",
            "fastai 2.8.5 requires scikit-learn, which is not installed.\n",
            "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, which is not installed.\n",
            "inequality 1.1.2 requires pandas>=2.1, which is not installed.\n",
            "esda 2.8.0 requires pandas>=2.1, which is not installed.\n",
            "esda 2.8.0 requires scikit-learn>=1.4, which is not installed.\n",
            "access 1.1.9 requires pandas>=0.23.4, which is not installed.\n",
            "db-dtypes 1.4.4 requires pandas>=1.5.3, which is not installed.\n",
            "librosa 0.11.0 requires scikit-learn>=1.1.0, which is not installed.\n",
            "imbalanced-learn 0.14.0 requires scikit-learn<2,>=1.4.2, which is not installed.\n",
            "pysal 25.7 requires pandas>=1.4, which is not installed.\n",
            "pysal 25.7 requires scikit-learn>=1.1, which is not installed.\n",
            "geopandas 1.1.1 requires pandas>=2.0.0, which is not installed.\n",
            "panel 1.8.4 requires pandas>=1.2, which is not installed.\n",
            "seaborn 0.13.2 requires pandas>=1.2, which is not installed.\n",
            "gradio 5.50.0 requires pandas<3.0,>=1.0, which is not installed.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "588191a58ceb42afac40ad1ac0c6a508"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas\n",
            "  Using cached pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
            "Collecting scikit-learn\n",
            "  Using cached scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
            "Collecting scikit-surprise\n",
            "  Using cached scikit_surprise-1.1.4-cp312-cp312-linux_x86_64.whl\n",
            "Using cached pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
            "Using cached scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (8.9 MB)\n",
            "Installing collected packages: scikit-surprise, scikit-learn, pandas\n",
            "Successfully installed pandas-2.3.3 scikit-learn-1.8.0 scikit-surprise-1.1.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas",
                  "sklearn",
                  "surprise"
                ]
              },
              "id": "c1251c5ac6fe41c09d3fa22965b4cc75"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_final= pd.read_csv(r\"/content/final_merged_data.csv\")\n",
        "df_final.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2WN8tIFZzk9",
        "outputId": "a0e246d3-c0ab-4f8d-8727-7c834050af16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(725639, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce Dataset\n",
        "# Keep active users (>= 20 ratings)\n",
        "user_counts = df_final['User-ID'].value_counts()\n",
        "active_users = user_counts[user_counts >= 20].index\n",
        "\n",
        "# Keep popular books (>= 20 ratings)\n",
        "book_counts = df_final['ISBN'].value_counts()\n",
        "popular_books = book_counts[book_counts >= 20].index\n",
        "\n",
        "df_small = df_final[\n",
        "    (df_final['User-ID'].isin(active_users)) &\n",
        "    (df_final['ISBN'].isin(popular_books))\n",
        "].copy()\n",
        "\n",
        "user_id_list = df_small[\"User-ID\"].unique().tolist()\n",
        "isbn_list    = df_small[\"ISBN\"].unique().tolist()\n",
        "\n",
        "print(\"Unique Users:\", len(user_id_list))\n",
        "print(\"Unique Books:\", len(isbn_list))\n",
        "\n",
        "\n",
        "print(\"Reduced data shape:\", df_small.shape)"
      ],
      "metadata": {
        "id": "SC9M7L_jZ9dZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bebc1ef-64ec-46e9-b705-d328a6aa25b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique Users: 5343\n",
            "Unique Books: 6694\n",
            "Reduced data shape: (268588, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build SVD Explicit Rating model\n",
        "reader = Reader(rating_scale=(0, 10))\n",
        "\n",
        "data = Dataset.load_from_df(\n",
        "    df_small[['User-ID', 'ISBN', 'Book-Rating']],\n",
        "    reader\n",
        ")\n",
        "trainset = data.build_full_trainset()\n",
        "\n",
        "svd = SVD()\n",
        "svd.fit(trainset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd0lzB8HtluJ",
        "outputId": "61e3c62c-54e3-4f99-f706-bec66e2ff943"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x7eef43d5a210>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build KNN Collaberative Filtering model\n",
        "sim_options = {\n",
        "    \"name\": \"pearson_baseline\", # Changed similarity metric to pearson_baseline\n",
        "    \"user_based\": True,\n",
        "    \"min_support\": 5\n",
        "}\n",
        "\n",
        "knn = KNNBaseline(sim_options=sim_options)\n",
        "knn.fit(trainset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x27onLn4uilO",
        "outputId": "ec2e4286-56a4-4c9f-bdba-a67abab33f9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimating biases using als...\n",
            "Computing the pearson_baseline similarity matrix...\n",
            "Done computing similarity matrix.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<surprise.prediction_algorithms.knns.KNNBaseline at 0x7eef31c90530>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF Content Based Model\n",
        "tfidf = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
        "tfidf_matrix = tfidf.fit_transform(df_small[\"Title_Clean_Lower\"].fillna(\"\"))"
      ],
      "metadata": {
        "id": "6QPnJ0YUuFKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Content Similarity Score Function\n",
        "def content_similarity(isbn1, isbn2):\n",
        "    try:\n",
        "        idx1 = df_small.index[df_small['ISBN'] == isbn1][0]\n",
        "        idx2 = df_small.index[df_small['ISBN'] == isbn2][0]\n",
        "        sim = linear_kernel(tfidf_matrix[idx1], tfidf_matrix[idx2]).flatten()[0]\n",
        "        return sim\n",
        "    except:\n",
        "        return 0"
      ],
      "metadata": {
        "id": "Hcf2a9_euyOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend_books_title(user_id, top_n=10):\n",
        "\n",
        "    # Books user has already rated\n",
        "    user_books = set(df_small[df_small[\"User-ID\"] == user_id][\"ISBN\"])\n",
        "\n",
        "    # All books available\n",
        "    all_books = df_small[\"ISBN\"].unique()\n",
        "\n",
        "    # For mapping ISBN â†’ Book Title\n",
        "    isbn_to_title = df_small[['ISBN', 'Book-Title']].drop_duplicates().set_index('ISBN')['Book-Title']\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for isbn in all_books:\n",
        "        if isbn in user_books:\n",
        "            continue\n",
        "\n",
        "        # 1. SVD score\n",
        "        try:\n",
        "            svd_score = svd.predict(user_id, isbn).est\n",
        "        except:\n",
        "            svd_score = 0\n",
        "\n",
        "        # 2. KNN CF score\n",
        "        try:\n",
        "            knn_score = knn.predict(user_id, isbn).est\n",
        "        except:\n",
        "            knn_score = 0\n",
        "\n",
        "        # 3. Content similarity score\n",
        "        try:\n",
        "            recent_books = list(user_books)[:5]\n",
        "            if len(recent_books) > 0:\n",
        "                cont_scores = [content_similarity(isbn, b) for b in recent_books]\n",
        "                cont_score = np.mean(cont_scores)\n",
        "            else:\n",
        "                cont_score = 0\n",
        "        except:\n",
        "            cont_score = 0\n",
        "\n",
        "        # Hybrid score\n",
        "        final_score = 0.5 * svd_score + 0.3 * knn_score + 0.2 * cont_score\n",
        "\n",
        "        # Append with title\n",
        "        title = isbn_to_title.get(isbn, \"Unknown Title\")\n",
        "\n",
        "        results.append((title, isbn, final_score))\n",
        "\n",
        "    # Sort results\n",
        "    results = sorted(results, key=lambda x: x[2], reverse=True)\n",
        "\n",
        "    return results[:top_n]"
      ],
      "metadata": {
        "id": "pcXVMNsF49Nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_user = df_small['User-ID'].sample(1).iloc[0]\n",
        "recommendations = recommend_books_title(test_user, top_n=10)\n",
        "\n",
        "for title, isbn, score in recommendations:\n",
        "    print(f\"{title}  |  ISBN: {isbn}  |  Score: {round(score, 3)}\")"
      ],
      "metadata": {
        "id": "3HP8m8eN4yNb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a630d81-8c55-4874-a1d7-349ca24fcee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pippi Longstocking (Seafarer Book)  |  ISBN: 0140309578  |  Score: 5.274\n",
            "The Other Boleyn Girl  |  ISBN: 0743227441  |  Score: 4.841\n",
            "All I Need to Know I Learned from My Cat  |  ISBN: 0894808249  |  Score: 4.716\n",
            "A Sand County Almanac (Outdoor Essays &amp; Reflections)  |  ISBN: 0345345053  |  Score: 3.99\n",
            "Belgarath the Sorcerer  |  ISBN: 0345403959  |  Score: 3.988\n",
            "The Curious Incident of the Dog in the Night-Time : A Novel  |  ISBN: 0385509456  |  Score: 3.987\n",
            "The Neverending Story  |  ISBN: 0140386335  |  Score: 3.958\n",
            "Einstein's Dreams  |  ISBN: 0446670111  |  Score: 3.936\n",
            "The Vanished Man : A Lincoln Rhyme Novel  |  ISBN: 0743222008  |  Score: 3.93\n",
            "Chicken Soup for the Teenage Soul II (Chicken Soup for the Soul Series)  |  ISBN: 1558746161  |  Score: 3.902\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save SVD model\n",
        "with open('/content/svd_model.pkl', 'wb') as f:\n",
        "    pickle.dump(svd, f)\n",
        "\n",
        "# Save KNN model\n",
        "with open('/content/knn_model.pkl', 'wb') as f:\n",
        "    pickle.dump(knn, f)\n",
        "\n",
        "# Save TF-IDF vectorizer\n",
        "with open('/content/tfidf_vectorizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tfidf, f)\n",
        "\n",
        "# Save df_small (the reduced dataset)\n",
        "with open('/content/df_small.pkl', 'wb') as f:\n",
        "    pickle.dump(df_small, f)\n",
        "\n",
        "print(\"Models and df_small saved as .pkl files in /content/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubAV97pa9dqy",
        "outputId": "935ae823-97cd-48ea-bf53-64e971dbe0ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models and df_small saved as .pkl files in /content/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JeXUjSqI9hMX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}